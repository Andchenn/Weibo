# Scrapy 爬取新浪微博 #

### 本节目标 ###

   本次爬取的目标是新浪微博用户的公开基本信息，如用户昵称、头像、用户的关注、粉丝列表以及发布的微博等，这些信息抓取之后保存至 MongoDB。

### 准备工作 ###
   
   代理池、Cookies 池实现并可以正常运行，安装 Scrapy、PyMongo 库。
 
### 爬取思路 ###

   首先，要实现用户的大规模爬取。这里采用的爬取方式是，以微博的几个大 V 为起始点，爬取他们各自的粉丝和关注列表，然后获取的粉丝和关注列表的粉丝和关注列表，以此类推，这样下去就可以实现递归爬取。如果一个用户与其他用户社交网络上的关联，那他们的信息就会被爬虫爬取到，这样就可以做到对所有用户的爬取。通过这样的方式，可以得到用户的唯一 ID，再根据 ID 获取每个用户发布的微博即可。

### 爬取站点 ###

   选取的爬取站点是：[微博](https://m.weibo.cn)